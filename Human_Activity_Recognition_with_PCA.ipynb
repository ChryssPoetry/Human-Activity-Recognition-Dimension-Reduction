# ---------------------------------------
# Human Activity Recognition with PCA
# Dimensionality Reduction + Classification
# ---------------------------------------

# Importing core libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Scikit-learn modules
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.feature_selection import VarianceThreshold
from sklearn.preprocessing import StandardScaler

# Display plots inline
%matplotlib inline

# ---------------------------------------
# Step 1: Load and Inspect the Data
# ---------------------------------------

# Load dataset
df = pd.read_csv("activity.csv")

# Display a sample of the data
print("Sample data:")
print(df.head())

# Check structure and types
print("\nDataset information:")
df.info()

# ---------------------------------------
# Step 2: Target Variable & Feature Matrix
# ---------------------------------------

# Store target variable (activity labels)
target = df['activity']

# Store feature matrix (exclude 'activity' column)
df_new = df.drop(columns=['activity'])

# ---------------------------------------
# Step 3: Train/Test Split
# ---------------------------------------

# Split data: 80% train / 20% test with fixed random seed for reproducibility
X_train, X_test, Y_train, Y_test = train_test_split(
    df_new, target, test_size=0.2, random_state=1234
)

# ---------------------------------------
# Step 4: Feature Selection with VarianceThreshold
# ---------------------------------------

# Remove features with low variance (<0.01) — likely uninformative
selector = VarianceThreshold(threshold=0.01)

# Fit on training set only
X_train_var = selector.fit_transform(X_train)

# Apply transformation to test set
X_test_var = selector.transform(X_test)

# Display the mask of retained features
print("\nSelected features after VarianceThreshold:")
print(selector.get_support())

# ---------------------------------------
# Step 5: Feature Scaling
# ---------------------------------------

# Normalize full original features
scaler_all = StandardScaler()
X_train = scaler_all.fit_transform(X_train)
X_test = scaler_all.transform(X_test)

# Normalize reduced-variance features
scaler_var = StandardScaler()
X_train_var = scaler_var.fit_transform(X_train_var)
X_test_var = scaler_var.transform(X_test_var)

# ---------------------------------------
# Step 6: Baseline Classifier on Full Data
# ---------------------------------------

# Fit k-NN classifier on original data (k=6)
knn = KNeighborsClassifier(n_neighbors=6)
knn.fit(X_train, Y_train)

# Evaluate performance
print("\nKNN with full features:")
print("Train Accuracy:", knn.score(X_train, Y_train))
print("Test Accuracy:", knn.score(X_test, Y_test))

# ---------------------------------------
# Step 7: Classifier on Reduced-Variance Data
# ---------------------------------------

knn_var = KNeighborsClassifier(n_neighbors=6)
knn_var.fit(X_train_var, Y_train)

print("\nKNN with variance-reduced features:")
print("Test Accuracy:", knn_var.score(X_test_var, Y_test))

# ---------------------------------------
# Step 8: PCA Analysis (Explained Variance)
# ---------------------------------------

# PCA without limiting number of components
pca = PCA()
X_train_pca = pca.fit_transform(X_train_var)

# Plot cumulative explained variance
plt.figure(figsize=(8, 4))
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlabel("Number of Principal Components")
plt.ylabel("Cumulative Explained Variance")
plt.title("Explained Variance vs Number of PCA Components")
plt.grid(True)
plt.show()

# Determine how many components are needed for 95% variance
n_components_95 = np.argmax(np.cumsum(pca.explained_variance_ratio_) >= 0.95) + 1
print(f"\nNumber of PCA components to retain 95% variance: {n_components_95}")

# ---------------------------------------
# Step 9: PCA Transformation (95% Variance)
# ---------------------------------------

pca_95 = PCA(n_components=n_components_95)
X_train_pca = pca_95.fit_transform(X_train_var)
X_test_pca = pca_95.transform(X_test_var)

# ---------------------------------------
# Step 10: Visualize PCA-Reduced Data
# ---------------------------------------

# 2D scatter plot (first 2 PCA components) for train and test
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
sns.scatterplot(x=X_train_pca[:, 0], y=X_train_pca[:, 1], hue=Y_train, palette="Set2", legend=False)
plt.title("PCA (Train Data)")

plt.subplot(1, 2, 2)
sns.scatterplot(x=X_test_pca[:, 0], y=X_test_pca[:, 1], hue=Y_test, palette="Set2", legend=False)
plt.title("PCA (Test Data)")

plt.suptitle("PCA: First 2 Principal Components")
plt.tight_layout()
plt.show()

# ---------------------------------------
# Step 11: Classifier on PCA-Reduced Data
# ---------------------------------------

knn_pca = KNeighborsClassifier(n_neighbors=6)
knn_pca.fit(X_train_pca, Y_train)

print("\nKNN with PCA-reduced features (95% variance):")
print("Test Accuracy:", knn_pca.score(X_test_pca, Y_test))

# ---------------------------------------
# Step 12: Score as Function of PCA Dimension
# ---------------------------------------

# Perform PCA keeping top 200 components
pca_200 = PCA(n_components=200)
X_train_pca_200 = pca_200.fit_transform(X_train_var)
X_test_pca_200 = pca_200.transform(X_test_var)

# Store accuracy scores for each number of components
scores = []

for k in range(1, 201):
    knn_k = KNeighborsClassifier(n_neighbors=6)
    knn_k.fit(X_train_pca_200[:, :k], Y_train)
    score = knn_k.score(X_test_pca_200[:, :k], Y_test)
    scores.append(score)

# ---------------------------------------
# Step 13: Plot Accuracy vs PCA Components
# ---------------------------------------

plt.figure(figsize=(10, 5))
plt.plot(range(1, 201), scores)
plt.xlabel("Number of PCA Components")
plt.ylabel("Test Accuracy")
plt.title("Model Accuracy vs PCA Dimensions")
plt.grid(True)
plt.show()

# ---------------------------------------
# Step 14: Best Score and Optimal Dimension
# ---------------------------------------

max_score = max(scores)
best_k = scores.index(max_score) + 1

print(f"\n✅ Maximum test accuracy: {max_score:.4f} at {best_k} PCA components")
